# 05 ä¿¡æ¯è®º (Information Theory)

## ä¸ºä»€ä¹ˆéœ€è¦ä¿¡æ¯è®ºï¼Ÿ

ä¿¡æ¯è®ºå¸®åŠ©æˆ‘ä»¬ç†è§£æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼š

### 1. äº¤å‰ç†µæŸå¤±
```python
# åˆ†ç±»ä»»åŠ¡æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°
loss = CrossEntropyLoss(predictions, targets)
# æœ¬è´¨æ˜¯ï¼šæœ€å°åŒ–é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„äº¤å‰ç†µ
```

### 2. KLæ•£åº¦
```python
# è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚
KL(P||Q) = é¢„æµ‹åˆ†å¸ƒQåç¦»çœŸå®åˆ†å¸ƒPçš„ç¨‹åº¦
# VAEã€çŸ¥è¯†è’¸é¦ã€å¼ºåŒ–å­¦ä¹ ä¸­å¤§é‡ä½¿ç”¨
```

### 3. ä¿¡æ¯é‡å’Œç†µ
- **ç†µ**ï¼šè¡¡é‡ä¸ç¡®å®šæ€§/ä¿¡æ¯é‡
- **äº’ä¿¡æ¯**ï¼šè¡¡é‡å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰

## æœ€å°å¿…è¦çŸ¥è¯†

### âœ… å¿…é¡»æŒæ¡
1. **ä¿¡æ¯é‡**ï¼šä¸€ä¸ªäº‹ä»¶åŒ…å«å¤šå°‘ä¿¡æ¯
2. **ç†µ**ï¼šéšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡
3. **äº¤å‰ç†µ**ï¼šç”¨Qåˆ†å¸ƒç¼–ç Påˆ†å¸ƒçš„å¹³å‡ä¿¡æ¯é‡
4. **KLæ•£åº¦**ï¼šä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚åº¦é‡
5. **äº¤å‰ç†µä¸KLæ•£åº¦çš„å…³ç³»**ï¼šä¸ºä»€ä¹ˆæœ€å°åŒ–äº¤å‰ç†µç­‰ä»·äºæœ€å°åŒ–KLæ•£åº¦

### âš ï¸ äº†è§£å³å¯
6. äº’ä¿¡æ¯ï¼ˆç‰¹å¾é€‰æ‹©ã€ä¿¡æ¯ç“¶é¢ˆç†è®ºï¼‰
7. ç›¸å¯¹ç†µçš„æ€§è´¨
8. æœ€å¤§ç†µåŸç†

### âŒ å¯ä»¥è·³è¿‡
- ä¿¡æºç¼–ç ã€ä¿¡é“å®¹é‡ç­‰é€šä¿¡ç†è®º
- å¤æ‚çš„ä¿¡æ¯è®ºè¯æ˜
- ç‡å¤±çœŸç†è®º

## å­¦ä¹ è·¯å¾„

```
ç¬¬1æ­¥ï¼šç†µ (40åˆ†é’Ÿ)
â”œâ”€â”€ docs/01_entropy.md
â””â”€â”€ notebooks/entropy_visualization.ipynb  # å¾…åˆ›å»º

ç¬¬2æ­¥ï¼šäº¤å‰ç†µå’ŒKLæ•£åº¦ (60åˆ†é’Ÿ)
â”œâ”€â”€ docs/02_cross_entropy_kl.md
â””â”€â”€ notebooks/loss_functions.ipynb

ç¬¬3æ­¥ï¼šäº’ä¿¡æ¯ (45åˆ†é’Ÿ)
â”œâ”€â”€ docs/03_mutual_information.md
â””â”€â”€ notebooks/mutual_information.ipynb  # å¾…åˆ›å»º
```

## ç›®å½•å†…å®¹

### ğŸ“„ docs/ - ç†è®ºæ–‡æ¡£
- `01_entropy.md` - ä¿¡æ¯é‡å’Œç†µçš„ç›´è§‚ç†è§£
- `02_cross_entropy_kl.md` - äº¤å‰ç†µå’ŒKLæ•£åº¦
- `03_mutual_information.md` - äº’ä¿¡æ¯åŠå…¶åº”ç”¨

### ğŸ’» notebooks/ - äº¤äº’å¼å®è·µ
- `entropy_visualization.ipynb` - ç†µçš„å¯è§†åŒ–
- `loss_functions.ipynb` - ä»ä¿¡æ¯è®ºè§’åº¦ç†è§£æŸå¤±å‡½æ•°
- `mutual_information.ipynb` - äº’ä¿¡æ¯è®¡ç®—å’Œåº”ç”¨

### ğŸ”§ code/ - å®ç”¨ä»£ç 
- `info_theory.py` - ä¿¡æ¯è®ºåº¦é‡è®¡ç®—å·¥å…·

## å¿«é€Ÿæµ‹è¯•

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è®¡ç®—ç¦»æ•£åˆ†å¸ƒçš„ç†µ
- [ ] ç†è§£äº¤å‰ç†µæŸå¤±çš„ä¿¡æ¯è®ºå«ä¹‰
- [ ] è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„KLæ•£åº¦
- [ ] è§£é‡Šä¸ºä»€ä¹ˆåˆ†ç±»ç”¨äº¤å‰ç†µï¼Œå›å½’ç”¨MSE
- [ ] ç”¨äº’ä¿¡æ¯è¯„ä¼°ç‰¹å¾é‡è¦æ€§

## ä¸æ·±åº¦å­¦ä¹ çš„è¿æ¥

| ä¿¡æ¯è®ºæ¦‚å¿µ | æ·±åº¦å­¦ä¹ åº”ç”¨ | PyTorchå®ç° |
|-----------|------------|-------------|
| äº¤å‰ç†µ | åˆ†ç±»æŸå¤± | `nn.CrossEntropyLoss()` |
| KLæ•£åº¦ | VAEæŸå¤±ã€çŸ¥è¯†è’¸é¦ | `F.kl_div()` |
| ç†µ | æ­£åˆ™åŒ–ã€ä¸ç¡®å®šæ€§ | æ‰‹åŠ¨è®¡ç®— |
| äº’ä¿¡æ¯ | ç‰¹å¾é€‰æ‹©ã€IBç†è®º | æ‰‹åŠ¨è®¡ç®— |
| æœ€å¤§ç†µ | Softmaxå‡½æ•° | `F.softmax()` |

## æ ¸å¿ƒå…¬å¼é€ŸæŸ¥

### ä¿¡æ¯é‡ï¼ˆè‡ªä¿¡æ¯ï¼‰
```
I(x) = -logâ‚‚ P(x)
å•ä½ï¼šbitï¼ˆä»¥2ä¸ºåº•ï¼‰æˆ– natï¼ˆä»¥eä¸ºåº•ï¼‰
```
ç›´è§‰ï¼šæ¦‚ç‡è¶Šå°çš„äº‹ä»¶ï¼Œä¿¡æ¯é‡è¶Šå¤§

### ç†µï¼ˆå¹³å‡ä¿¡æ¯é‡ï¼‰
```
H(X) = -Î£ P(x) log P(x)
     = E[-log P(x)]
```
ç›´è§‰ï¼šåˆ†å¸ƒè¶Šå‡åŒ€ï¼Œç†µè¶Šå¤§ï¼ˆä¸ç¡®å®šæ€§è¶Šé«˜ï¼‰

### äº¤å‰ç†µ
```
H(P, Q) = -Î£ P(x) log Q(x)
```
ç›´è§‰ï¼šç”¨åˆ†å¸ƒQç¼–ç åˆ†å¸ƒPæ‰€éœ€çš„å¹³å‡æ¯”ç‰¹æ•°

### KLæ•£åº¦ï¼ˆç›¸å¯¹ç†µï¼‰
```
KL(P||Q) = Î£ P(x) log[P(x)/Q(x)]
         = H(P, Q) - H(P)
```
ç›´è§‰ï¼šQåˆ†å¸ƒåç¦»Påˆ†å¸ƒçš„ç¨‹åº¦

**é‡è¦æ€§è´¨**ï¼š
- KL(P||Q) â‰¥ 0
- KL(P||Q) â‰  KL(Q||P)ï¼ˆä¸å¯¹ç§°ï¼‰
- KL(P||Q) = 0 å½“ä¸”ä»…å½“ P = Q

### äº’ä¿¡æ¯
```
I(X;Y) = KL(P(X,Y) || P(X)P(Y))
       = H(X) + H(Y) - H(X,Y)
```
ç›´è§‰ï¼šXå’ŒYå…±äº«çš„ä¿¡æ¯é‡

## æŸå¤±å‡½æ•°çš„ä¿¡æ¯è®ºè§£é‡Š

### äºŒåˆ†ç±»äº¤å‰ç†µ
```python
# PyTorch
loss = nn.BCELoss()(predictions, targets)

# ä¿¡æ¯è®ºè§£é‡Š
loss = -[y*log(Å·) + (1-y)*log(1-Å·)]
     = H(y, Å·)  # çœŸå®åˆ†å¸ƒyå’Œé¢„æµ‹åˆ†å¸ƒÅ·çš„äº¤å‰ç†µ
```

### å¤šåˆ†ç±»äº¤å‰ç†µ
```python
# PyTorch
loss = nn.CrossEntropyLoss()(logits, targets)

# ä¿¡æ¯è®ºè§£é‡Š
loss = -Î£ y_i * log(softmax(logits)_i)
     = H(y, Å·) where yæ˜¯one-hotåˆ†å¸ƒ
```

### ä¸ºä»€ä¹ˆæœ€å°åŒ–äº¤å‰ç†µï¼Ÿ
```
min H(P, Q) = min [H(P) + KL(P||Q)]
   Q             Q

å› ä¸ºH(P)æ˜¯å¸¸æ•°ï¼Œæ‰€ä»¥ï¼š
min H(P, Q) âŸº min KL(P||Q)
   Q             Q
```
æœ€å°åŒ–äº¤å‰ç†µ = è®©é¢„æµ‹åˆ†å¸ƒæ¥è¿‘çœŸå®åˆ†å¸ƒ

## å®è·µç¤ºä¾‹

### è®¡ç®—ç†µ
```python
import numpy as np

def entropy(probs):
    """è®¡ç®—ç¦»æ•£åˆ†å¸ƒçš„ç†µ"""
    return -np.sum(probs * np.log2(probs + 1e-10))

# å‡åŒ€åˆ†å¸ƒï¼šç†µæœ€å¤§
p_uniform = np.array([0.25, 0.25, 0.25, 0.25])
print(entropy(p_uniform))  # 2.0 bits

# ç¡®å®šåˆ†å¸ƒï¼šç†µä¸º0
p_certain = np.array([1.0, 0.0, 0.0, 0.0])
print(entropy(p_certain))  # 0.0 bits
```

### è®¡ç®—KLæ•£åº¦
```python
def kl_divergence(p, q):
    """è®¡ç®—KL(P||Q)"""
    return np.sum(p * np.log(p / (q + 1e-10) + 1e-10))

p = np.array([0.5, 0.5])
q1 = np.array([0.5, 0.5])
q2 = np.array([0.9, 0.1])

print(kl_divergence(p, q1))  # 0.0 (ç›¸åŒåˆ†å¸ƒ)
print(kl_divergence(p, q2))  # > 0 (ä¸åŒåˆ†å¸ƒ)
```

## æ¨èèµ„æº

### è§†é¢‘
- [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/) - å¯è§†åŒ–æ•™ç¨‹
- [StatQuest - Cross Entropy](https://www.youtube.com/watch?v=6ArSys5qHAU)

### æ–‡ç« 
- [Understanding Cross-Entropy Loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)
- [KL Divergence for Machine Learning](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

### ä¹¦ç±
- ã€ŠElements of Information Theoryã€‹(Cover & Thomas) - ç¬¬2ç« 

## å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆåˆ†ç±»ç”¨äº¤å‰ç†µï¼Œå›å½’ç”¨MSEï¼Ÿ**
A: åˆ†ç±»è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”¨äº¤å‰ç†µè¡¡é‡åˆ†å¸ƒå·®å¼‚ï¼›å›å½’è¾“å‡ºè¿ç»­å€¼ï¼Œå‡è®¾é«˜æ–¯å™ªå£°ï¼Œæœ€å¤§ä¼¼ç„¶æ¨å¯¼å‡ºMSEã€‚

**Q: KLæ•£åº¦ä¸ºä»€ä¹ˆä¸å¯¹ç§°ï¼Ÿ**
A: KL(P||Q)è¡¡é‡ç”¨Qè¿‘ä¼¼Pçš„ä»£ä»·ï¼ŒKL(Q||P)è¡¡é‡ç”¨Pè¿‘ä¼¼Qçš„ä»£ä»·ï¼Œä¸¤è€…ä¸åŒã€‚

**Q: äº’ä¿¡æ¯åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰ä»€ä¹ˆç”¨ï¼Ÿ**
A: ç‰¹å¾é€‰æ‹©ã€ä¿¡æ¯ç“¶é¢ˆç†è®ºï¼ˆIBï¼‰ã€å¯¹æ¯”å­¦ä¹ ä¸­çš„ç›®æ ‡å‡½æ•°ã€‚

## ä¸‹ä¸€æ­¥

å®Œæˆä¿¡æ¯è®ºåï¼Œå‰å¾€ [06_rl_math](../06_rl_math/) å­¦ä¹ å¼ºåŒ–å­¦ä¹ ä¸“ç”¨æ•°å­¦ï¼Œæˆ–è€…è¿›å…¥ [07_applications/dl_examples](../07_applications/dl_examples) è¿›è¡Œæ·±åº¦å­¦ä¹ ç»¼åˆå®è·µã€‚
