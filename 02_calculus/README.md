# 02 å¾®ç§¯åˆ† (Calculus)

## ä¸ºä»€ä¹ˆéœ€è¦å¾®ç§¯åˆ†ï¼Ÿ

å¾®ç§¯åˆ†æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒï¼š

### 1. åå‘ä¼ æ’­ (Backpropagation)
ç¥ç»ç½‘ç»œè®­ç»ƒçš„æœ¬è´¨æ˜¯**æœ€å°åŒ–æŸå¤±å‡½æ•°**ï¼Œéœ€è¦è®¡ç®—æŸå¤±å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼š
```python
# æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è°ƒæ•´å‚æ•°æ¥å‡å°‘æŸå¤±
âˆ‚Loss/âˆ‚wâ‚, âˆ‚Loss/âˆ‚wâ‚‚, ..., âˆ‚Loss/âˆ‚wâ‚™
```

### 2. æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
```python
# å‚æ•°æ›´æ–°å…¬å¼
w_new = w_old - learning_rate * âˆ‚Loss/âˆ‚w
```

### 3. é“¾å¼æ³•åˆ™
æ·±åº¦ç½‘ç»œæ˜¯å±‚å±‚åµŒå¥—çš„å¤åˆå‡½æ•°ï¼Œåå‘ä¼ æ’­ä¾èµ–é“¾å¼æ³•åˆ™ï¼š
```
Loss = f(g(h(x)))
âˆ‚Loss/âˆ‚x = (âˆ‚f/âˆ‚g) Ã— (âˆ‚g/âˆ‚h) Ã— (âˆ‚h/âˆ‚x)
```

## æœ€å°å¿…è¦çŸ¥è¯†

### âœ… å¿…é¡»æŒæ¡
1. **å¯¼æ•°çš„ç›´è§‚ç†è§£**ï¼šå‡½æ•°åœ¨æŸç‚¹çš„å˜åŒ–ç‡
2. **åå¯¼æ•°**ï¼šå¤šå…ƒå‡½æ•°å¯¹æŸä¸€ä¸ªå˜é‡çš„å¯¼æ•°
3. **é“¾å¼æ³•åˆ™**ï¼šå¤åˆå‡½æ•°æ±‚å¯¼ï¼ˆåå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€ï¼‰
4. **æ¢¯åº¦**ï¼šå¤šå…ƒå‡½æ•°çš„æ‰€æœ‰åå¯¼æ•°ç»„æˆçš„å‘é‡
5. **æ¢¯åº¦ä¸‹é™**ï¼šæ²¿ç€æ¢¯åº¦åæ–¹å‘æ›´æ–°å‚æ•°

### âš ï¸ äº†è§£å³å¯
6. å¸¸è§å‡½æ•°çš„å¯¼æ•°ï¼ˆReLUã€Sigmoidã€Softmaxï¼‰
7. é›…å¯æ¯”çŸ©é˜µå’Œæµ·æ£®çŸ©é˜µï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰

### âŒ å¯ä»¥è·³è¿‡
- ç§¯åˆ†çš„è¯¦ç»†è®¡ç®—ï¼ˆDLä¸­å¾ˆå°‘ç›´æ¥ç”¨ï¼‰
- å¾®åˆ†æ–¹ç¨‹æ±‚è§£
- æ³°å‹’å±•å¼€çš„é«˜é˜¶é¡¹

## å­¦ä¹ è·¯å¾„

```
ç¬¬1æ­¥ï¼šå¯¼æ•°å’Œåå¯¼æ•° (40åˆ†é’Ÿ)
â”œâ”€â”€ docs/01_derivatives.md
â””â”€â”€ notebooks/derivatives_visualization.ipynb  # å¾…åˆ›å»º

ç¬¬2æ­¥ï¼šé“¾å¼æ³•åˆ™ (50åˆ†é’Ÿ)
â”œâ”€â”€ docs/02_chain_rule.md
â””â”€â”€ notebooks/backpropagation.ipynb

ç¬¬3æ­¥ï¼šæ¢¯åº¦ä¸‹é™ (60åˆ†é’Ÿ)
â”œâ”€â”€ docs/03_gradient_descent.md
â””â”€â”€ notebooks/gradient_visualization.ipynb  # å¾…åˆ›å»º
```

## ç›®å½•å†…å®¹

### ğŸ“„ docs/ - ç†è®ºæ–‡æ¡£
- `01_derivatives.md` - å¯¼æ•°å’Œåå¯¼æ•°çš„ç›´è§‚ç†è§£
- `02_chain_rule.md` - é“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­
- `03_gradient_descent.md` - æ¢¯åº¦ä¸‹é™åŠå…¶å˜ä½“

### ğŸ’» notebooks/ - äº¤äº’å¼å®è·µ
- `derivatives_visualization.ipynb` - å¯¼æ•°çš„å‡ ä½•æ„ä¹‰å¯è§†åŒ–
- `backpropagation.ipynb` - æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­
- `gradient_visualization.ipynb` - æ¢¯åº¦ä¸‹é™è¿‡ç¨‹å¯è§†åŒ–

### ğŸ”§ code/ - å®ç”¨ä»£ç 
- `autodiff.py` - ç®€å•çš„è‡ªåŠ¨å¾®åˆ†å®ç°

## å¿«é€Ÿæµ‹è¯•

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] æ‰‹åŠ¨è®¡ç®—ç®€å•å‡½æ•°çš„å¯¼æ•°å’Œåå¯¼æ•°
- [ ] ç”¨é“¾å¼æ³•åˆ™æ¨å¯¼ä¸¤å±‚ç¥ç»ç½‘ç»œçš„æ¢¯åº¦
- [ ] å®ç°ç®€å•çš„æ¢¯åº¦ä¸‹é™ç®—æ³•
- [ ] è§£é‡Šä¸ºä»€ä¹ˆå­¦ä¹ ç‡å¤ªå¤§ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®š

## ä¸æ·±åº¦å­¦ä¹ çš„è¿æ¥

| å¾®ç§¯åˆ†æ¦‚å¿µ | æ·±åº¦å­¦ä¹ åº”ç”¨ | ç¤ºä¾‹ä»£ç ä½ç½® |
|----------|------------|------------|
| åå¯¼æ•° | è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ | æ¯æ¬¡å‚æ•°æ›´æ–° |
| é“¾å¼æ³•åˆ™ | åå‘ä¼ æ’­ç®—æ³• | `loss.backward()` |
| æ¢¯åº¦ä¸‹é™ | å‚æ•°ä¼˜åŒ– | `optimizer.step()` |
| å­¦ä¹ ç‡è°ƒæ•´ | å­¦ä¹ ç‡è¡°å‡ | `lr_scheduler` |

## æ ¸å¿ƒå…¬å¼é€ŸæŸ¥

### å¸¸è§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°

**Sigmoid**:
```
Ïƒ(x) = 1 / (1 + e^(-x))
Ïƒ'(x) = Ïƒ(x) Ã— (1 - Ïƒ(x))
```

**ReLU**:
```
ReLU(x) = max(0, x)
ReLU'(x) = 1 if x > 0, else 0
```

**Tanh**:
```
tanh'(x) = 1 - tanhÂ²(x)
```

### æ¢¯åº¦ä¸‹é™å˜ä½“

**SGD**: `w = w - Î· Ã— âˆ‡L`

**Momentum**: `v = Î²Ã—v + âˆ‡L; w = w - Î·Ã—v`

**Adam**: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡

## æ¨èèµ„æº

### è§†é¢‘
- [3Blue1Brown - å¾®ç§¯åˆ†çš„æœ¬è´¨](https://www.3blue1brown.com/topics/calculus) ç³»åˆ—
- [StatQuest - Gradient Descent](https://www.youtube.com/watch?v=sDv4f4s2SB8)

### æ–‡ç« 
- [CS231n - Backpropagation](http://cs231n.github.io/optimization-2/)
- [Calculus on Computational Graphs](http://colah.github.io/posts/2015-08-Backprop/)

### äº¤äº’å¼å·¥å…·
- [TensorFlow Playground](https://playground.tensorflow.org/) - å¯è§†åŒ–ç¥ç»ç½‘ç»œè®­ç»ƒ

## ä¸‹ä¸€æ­¥

å®Œæˆå¾®ç§¯åˆ†åï¼Œå‰å¾€ [03_probability_statistics](../03_probability_statistics/) å­¦ä¹ æ¦‚ç‡ç»Ÿè®¡ï¼Œç†è§£ä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚
